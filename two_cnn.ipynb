{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tfrecorder import TFrecorder\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# input_fn's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfr = TFrecorder()\n",
    "def input_fn_maker(path, data_info_path, shuffle=False, batch_size = 1, epoch = 1, padding = None):\n",
    "    def input_fn():\n",
    "        filenames = tfr.get_filenames(path=path, shuffle=shuffle)\n",
    "        dataset=tfr.get_dataset(paths=filenames, data_info=data_info_path, shuffle = shuffle, \n",
    "                            batch_size = batch_size, epoch = epoch, padding =padding)\n",
    "        iterator = dataset.make_one_shot_iterator()\n",
    "        return iterator.get_next()\n",
    "    return input_fn\n",
    "\n",
    "padding_info = ({'image':[784,],'label':[]})\n",
    "test_input_fn = input_fn_maker('mnist_tfrecord/test/',  'mnist_tfrecord/data_info.csv',batch_size = 512,\n",
    "                               padding = padding_info)\n",
    "train_input_fn = input_fn_maker('mnist_tfrecord/train/',  'mnist_tfrecord/data_info.csv', shuffle=True, batch_size = 128,\n",
    "                               padding = padding_info)\n",
    "train_eval_fn = input_fn_maker('mnist_tfrecord/train/',  'mnist_tfrecord/data_info.csv', batch_size = 512,\n",
    "                               padding = padding_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fn(features, mode):\n",
    "    #Hparams\n",
    "    relation_activation = tf.nn.sigmoid\n",
    "    # inputs\n",
    "    # reshape 784维的图片到28x28的平面表达，1为channel数\n",
    "    \n",
    "    old_image = tf.reshape(features['image'],[-1,28,28,1])\n",
    "    new_image = old_image\n",
    "    # labels\n",
    "    old_label = features['label']\n",
    "    new_label = features['label']\n",
    "    \n",
    "    #old_image = tf.reshape(features['old_image'],[-1,28,28,1])\n",
    "    #new_image = tf.reshape(features['new_image'],[-1,28,28,1])\n",
    "    # labels\n",
    "    #old_label = features['old_label']\n",
    "    #new_label = features['new_label']\n",
    "    \n",
    "    # shape: [None,28,28,1]\n",
    "    old_conv1 = tf.layers.conv2d(\n",
    "        inputs=old_image,\n",
    "        filters=32,\n",
    "        kernel_size=[5, 5],\n",
    "        padding=\"same\",\n",
    "        activation=tf.nn.relu,\n",
    "        name = 'conv1',reuse=tf.AUTO_REUSE)# share weight for old_image and new_image\n",
    "    \n",
    "    new_conv1 = tf.layers.conv2d(\n",
    "        inputs=new_image,\n",
    "        filters=32,\n",
    "        kernel_size=[5, 5],\n",
    "        padding=\"same\",\n",
    "        activation=tf.nn.relu,\n",
    "        name = 'conv1',reuse=tf.AUTO_REUSE)\n",
    "    \n",
    "    # shape: [None,28,28,32]\n",
    "    old_pool1 = tf.layers.max_pooling2d(inputs=old_conv1, pool_size=[2, 2], strides=2)\n",
    "    new_pool1 = tf.layers.max_pooling2d(inputs=new_conv1, pool_size=[2, 2], strides=2)\n",
    "    # shape: [None,14,14,32]\n",
    "    \n",
    "    relation1_input = tf.layers.flatten(tf.concat(values=[old_pool1, new_pool1], axis=-1))\n",
    "    \n",
    "    relation1 = tf.layers.dense(inputs=relation1_input, units=1, activation=relation_activation, name= 'relation1')\n",
    "    \n",
    "    \n",
    "    old_conv2 = tf.layers.conv2d(\n",
    "        inputs= old_pool1,\n",
    "        filters=64,\n",
    "        kernel_size=[5, 5],\n",
    "        padding=\"same\",\n",
    "        activation=tf.nn.relu,\n",
    "        name = 'conv2',reuse=tf.AUTO_REUSE)\n",
    "    \n",
    "    new_conv2 = tf.layers.conv2d(\n",
    "        inputs= new_pool1,\n",
    "        filters=64,\n",
    "        kernel_size=[5, 5],\n",
    "        padding=\"same\",\n",
    "        activation=tf.nn.relu,\n",
    "        name = 'conv2',reuse=tf.AUTO_REUSE)\n",
    "    \n",
    "    # shape: [None,14,14,64]\n",
    "    old_pool2 = tf.layers.max_pooling2d(inputs=old_conv2, pool_size=[2, 2], strides=2)\n",
    "    new_pool2 = tf.layers.max_pooling2d(inputs=new_conv2, pool_size=[2, 2], strides=2)\n",
    "    \n",
    "    # shape: [None,7,7,64]\n",
    "    old_pool2_flat = tf.reshape(old_pool2, [-1, 7 * 7 * 64],name='re1')\n",
    "    new_pool2_flat = tf.reshape(new_pool2, [-1, 7 * 7 * 64],name='re2')\n",
    "\n",
    "    relation2_input = tf.concat(values=[old_pool2_flat, new_pool2_flat], axis=-1)\n",
    "    relation2 = tf.layers.dense(inputs=relation2_input, units=1, activation=relation_activation, name= 'relation2')\n",
    "    \n",
    "    # shape: [None,3136]\n",
    "    old_dense1 = tf.layers.dense(inputs=old_pool2_flat, units=1024, activation=tf.nn.relu, name= 'dense1',reuse=tf.AUTO_REUSE)\n",
    "    new_dense1 = tf.layers.dense(inputs=new_pool2_flat, units=1024, activation=tf.nn.relu, name= 'dense1',reuse=tf.AUTO_REUSE)\n",
    "    \n",
    "    # shape: [None,1024]\n",
    "    old_logits = tf.layers.dense(inputs=old_dense1, units=10, name= 'output',reuse=tf.AUTO_REUSE)\n",
    "    new_logits = tf.layers.dense(inputs=new_dense1, units=10, name= 'output',reuse=tf.AUTO_REUSE)\n",
    "    \n",
    "    # shape: [None,10]\n",
    "    \n",
    "    predictions = {\n",
    "        \"old_image\":old_image,\n",
    "        \"new_image\":new_image,\n",
    "        \"old_label\": old_label,\n",
    "        \"new_label\": new_label,\n",
    "        \"relation1\":relation1,\n",
    "        \"relation2\":relation2,\n",
    "        \"old_class\": tf.argmax(input=old_logits, axis=1),\n",
    "        \"new_class\": tf.argmax(input=new_logits, axis=1),\n",
    "        \"old_probability\": tf.nn.softmax(old_logits, name=\"old_softmax_tensor\"),\n",
    "        \"new_probability\": tf.nn.softmax(new_logits, name=\"new_softmax_tensor\")\n",
    "        }\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions)\n",
    "    \n",
    "    old_loss = tf.losses.sparse_softmax_cross_entropy(labels=old_label, logits=old_logits,reduction=tf.losses.Reduction.NONE)\n",
    "    new_loss = tf.losses.sparse_softmax_cross_entropy(labels=new_label, logits=new_logits,reduction=tf.losses.Reduction.NONE)\n",
    "    \n",
    "    relation = tf.reshape(relation1+relation2,[-1])\n",
    "    \n",
    "    loss = tf.reduce_mean(old_loss*relation + new_loss*(1-relation),axis=0)\n",
    "    \n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=1e-3)\n",
    "        train_op = optimizer.minimize(\n",
    "        loss=loss,\n",
    "        global_step=tf.train.get_global_step())\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)\n",
    "    eval_metric_ops = {\"old_accuracy\": tf.metrics.accuracy(labels=old_label, predictions=predictions[\"old_class\"]),\n",
    "                       \"new_accuracy\": tf.metrics.accuracy(labels=new_label, predictions=predictions[\"new_class\"])}\n",
    "    return tf.estimator.EstimatorSpec(mode=mode, loss=loss, eval_metric_ops=eval_metric_ops)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': 'two', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f74f56549b0>, '_task_type': 'worker', '_task_id': 0, '_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
     ]
    }
   ],
   "source": [
    "mnist_classifier = tf.estimator.Estimator(\n",
    "    model_fn=model_fn, model_dir=\"two\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read dataframe from mnist_tfrecord/train/train50000_60000.tfrecord x 6\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into two/model.ckpt.\n",
      "INFO:tensorflow:loss = 2.31093, step = 1\n",
      "INFO:tensorflow:global_step/sec: 5.36911\n",
      "INFO:tensorflow:loss = 0.109293, step = 101 (18.627 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.4234\n",
      "INFO:tensorflow:loss = 0.122745, step = 201 (18.439 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.47068\n",
      "INFO:tensorflow:loss = 0.0295126, step = 301 (18.280 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.45582\n",
      "INFO:tensorflow:loss = 0.0409706, step = 401 (18.328 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 430 into two/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.0395434.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.estimator.estimator.Estimator at 0x7f74f5654be0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist_classifier.train(\n",
    "    input_fn=train_input_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# evaluate train set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read dataframe from mnist_tfrecord/train/train0_10000.tfrecord x 6\n",
      "INFO:tensorflow:Starting evaluation at 2018-05-06-08:27:04\n",
      "INFO:tensorflow:Restoring parameters from two/model.ckpt-430\n",
      "INFO:tensorflow:Finished evaluation at 2018-05-06-08:27:17\n",
      "INFO:tensorflow:Saving dict for global step 430: global_step = 430, loss = 0.039941, new_accuracy = 0.988345, old_accuracy = 0.988345\n",
      "train set\n",
      "{'loss': 0.03994102, 'new_accuracy': 0.98834544, 'old_accuracy': 0.98834544, 'global_step': 430}\n"
     ]
    }
   ],
   "source": [
    "eval_results = mnist_classifier.evaluate(input_fn=train_eval_fn)\n",
    "print('train set')\n",
    "print(eval_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# evaluate test set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read dataframe from mnist_tfrecord/test/test0_10000.tfrecord x 1\n",
      "INFO:tensorflow:Starting evaluation at 2018-05-06-08:27:17\n",
      "INFO:tensorflow:Restoring parameters from two/model.ckpt-430\n",
      "INFO:tensorflow:Finished evaluation at 2018-05-06-08:27:20\n",
      "INFO:tensorflow:Saving dict for global step 430: global_step = 430, loss = 0.0394304, new_accuracy = 0.9874, old_accuracy = 0.9874\n",
      "test set\n",
      "{'loss': 0.039430376, 'new_accuracy': 0.9874, 'old_accuracy': 0.9874, 'global_step': 430}\n"
     ]
    }
   ],
   "source": [
    "eval_results = mnist_classifier.evaluate(input_fn=test_input_fn)\n",
    "print('test set')\n",
    "print(eval_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read dataframe from mnist_tfrecord/test/test0_10000.tfrecord x 1\n",
      "WARNING:tensorflow:Input graph does not contain a QueueRunner. That means predict yields forever. This is probably a mistake.\n",
      "INFO:tensorflow:Restoring parameters from two/model.ckpt-430\n"
     ]
    }
   ],
   "source": [
    "predicts =list(mnist_classifier.predict(input_fn=test_input_fn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# print predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.49374059], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicts[0]['relation1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.48175532], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicts[0]['relation2']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
